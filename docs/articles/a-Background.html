<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="WhatsMatching">
<title>Vignette 1 - Background • WhatsMatching</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.3/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.3/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Vignette 1 - Background">
<meta property="og:description" content="WhatsMatching">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">WhatsMatching</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/a-Background.html">Vignette 1 - Background</a>
    <a class="dropdown-item" href="../articles/b-TheApp.html">Vignette 2 - The App</a>
    <a class="dropdown-item" href="../articles/c-Applications.html">Vignette 3 - Applications</a>
    <a class="dropdown-item" href="../articles/d-References.html">References</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/jmeyer2482/WhatsMatching/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Vignette 1 - Background</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/jmeyer2482/WhatsMatching/blob/HEAD/vignettes/a-Background.Rmd" class="external-link"><code>vignettes/a-Background.Rmd</code></a></small>
      <div class="d-none name"><code>a-Background.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<div class="section level3">
<h3 id="the-problem-of-observational-data">The problem of observational data<a class="anchor" aria-label="anchor" href="#the-problem-of-observational-data"></a>
</h3>
<p>In observational data there is no ability to control which people
receive treatment and which do not. In fact, there is often little
control over the available covariates that may help inform the outcomes.
This potentially exposes observational data to selection bias and
confounding.</p>
<p>Observational data has long been used to investigate effects in the
real world. Thanks to computers and the internet the volume of
observational data available has exploded and, while it is excellent to
have all this data, making inferences from it should be approached with
caution. Why is that? Well, it’s because using data that hasn’t been
generated with the purpose of proving your hypothesis isn’t likely to
have all the information required to make an accurate estimate of the
treatment effect and may include data that will confound the
results.</p>
<p>When using observational data to make causal inferences regarding
some sort of “treatment”, the analyst is looking for the randomised
experiment within the data. However, observational data from many
sources is not designed for this purpose and often doesn’t contain all
the information required or contains a sample size that is larger and
not necessarily representative of the group of interest, or both. This
has led to the development of methods to assist in the pre-processing of
data to obtain the most accurate results possible.</p>
</div>
<div class="section level3">
<h3 id="approaches-to-dealing-with-observational-data">Approaches to dealing with observational data<a class="anchor" aria-label="anchor" href="#approaches-to-dealing-with-observational-data"></a>
</h3>
<p>There are a number of established approaches to dealing with
observational data. The primary aim of these methods is to reduce bias
in the estimation of the casual effect, especially confounding. Some
options may include:<br>
- restricting the data on the confounding covariate to one level/group,
for example females - propensity score weighting uses the inverse
probability of treatment as an adjustment on the model - stratifying can
be used on the available data to analyse groups separately then average
the effect over the groups, equally this can be done with the propensity
score<br>
- matching pairs treated and untreated individuals within the data that
are “close” together - multivariate regression includes the covariates
in the regression model for estimating the effect</p>
<p>We will be looking at matching methods and methods that use the
propensity score.</p>
</div>
</div>
<div class="section level2">
<h2 id="matching-methods">Matching methods<a class="anchor" aria-label="anchor" href="#matching-methods"></a>
</h2>
<p>Matching is a methodology that is used in the analysis of
observational data. Matching is used as a pre-processing step to assist
in improving the estimate of a given causal effect on an outcome of
interest. The causal variable is generally framed as a binary treatment.
Matching occurs by pairing the individuals, or units, in the dataset
across the treated and control (untreated) groups. The purpose of
pairing, or matching, the data is to find a hidden randomised experiment
within the data. That is to say, to find data within the dataset that is
free from confounding and other bias that is usually automatically
accounted for by selection in a true randomised experiment.</p>
<p>Essentially, when matching is conducted, a type of distance is used
to see how far apart all the treated units are from all the controls.
Then an iterative process takes place whereby each unit is matched to
its “neighbour”. This process can occur in an order of the analysts
choosing and with or without replacement. At the end, the remaining
units that are unmatched are removed and the analysis occurs on the
matched data which, hopefully, is the hidden experiment being looked
for.</p>
<div class="section level3">
<h3 id="propensity-score">Propensity score<a class="anchor" aria-label="anchor" href="#propensity-score"></a>
</h3>
<p>Rosenbaum and Rubin (1983) defined the propensity score as the
“conditional probability of assignment to a particular treatment given a
vector of observed covariates” 1. It is one of the most widely used
statistical methods when analysing observational data2,3. In most cases,
the propensity score is estimated from available data using a simple
logistic regression on a binary treatment assignment indicator. However,
the estimation method is at the discretion of the analyst and is not
limited to logistic regression.</p>
<p>The propensity score allows the reduction of a multidimensional space
to a single dimension for easy comparison between treated and untreated
units. Mathematically, it can be formulated as <span class="math inline">\(e(x_i) = pr(Z_i=1|x)\)</span> where <span class="math inline">\(e(x_i)\)</span> is the propensity score, <span class="math inline">\(Z\)</span> is the treatment variable, <span class="math inline">\(x\)</span> is a vector of covariates, and <span class="math inline">\(i\)</span> represents an individual1,3. Therefore,
the propensity score is the probability of receiving treatment given
covariates <span class="math inline">\(x\)</span>.</p>
<p>The objective of using the propensity score in data analysis is to
replicate a randomised experiment within observational data as a means
of bias reduction and covariate balancing. The propensity score can be
used in multiple way to achieve this, notably matching, weighting and
stratification (aka subclassification)2,4.<br>
The propensity score is also widely used for other methods to improve
estimates including weighting and stratification. While these methods
are not the primary purpose of the app, their outcomes have been
provided in the outputs to give some further insights into what
alternative methods may demonstrate.</p>
</div>
<div class="section level3">
<h3 id="applications-of-propensity-scores-matching-weighting-and-stratification">Applications of propensity scores – matching, weighting and
stratification<a class="anchor" aria-label="anchor" href="#applications-of-propensity-scores-matching-weighting-and-stratification"></a>
</h3>
<div class="section level4">
<h4 id="matching">Matching<a class="anchor" aria-label="anchor" href="#matching"></a>
</h4>
<p>The propensity score is the most common way of conducting matching.
It reduces a multidimensional space to a single dimension which allows
for easy matching between units. It is based on theory by Rosenbaum and
Rubin[ref] and relies on logistic regression to determine the
probability that a unit will be treated. Mathematically, <span class="math inline">\(e(x_i) = pr(Z_i=1|x)\)</span> where <span class="math inline">\(e(x_i)\)</span> is the propensity score, <span class="math inline">\(Z\)</span> is the treatment variable, <span class="math inline">\(x\)</span> is a vector of covariates, and <span class="math inline">\(i\)</span> represents an individual.</p>
<p>Many packages in <code>R</code> that perform matching analysis take
the leg work out of calculating the type of distance being used. When
calculating the propensity score for matching the logistic regression
formula looks like <span class="math inline">\(t \sim x_1 + x_2 + ... +
x_n\)</span> where <span class="math inline">\(t\)</span> is the
treatment variable and <span class="math inline">\(x\)</span> represents
a chosen covariate to match on. Once the probabilities have been
calculated then treated units are paired with control units that have
the same or similar probability of treatment.</p>
<p>Matching requires that a type of distance between units is
calculated. This distance is then used to pair or match units that are
close together while discarding or pruning those that are not. Once
pruning has occurred to the satisfaction of the analyst, the treatment
effect can be estimated on the pruned dataset. Many methods of matching
have been explored in the literature2,3 and while the propensity score
is perhaps the most used distance metric, other distance metrics are
available, including the Mahalanobis distance.</p>
<p>It is important to note that estimates should generally be based on
the average treatment effect on the treated (ATT) when using matching
methods. This makes sense intuitively as matching effectively removes
individuals from the data that could not be considered both treated and
untreated. This is referred to as strongly ignorable treatment
assignment and is a key assumption for providing unbiased estimates with
the propensity score1.</p>
<p>PSM is usually conducted using nearest neighbour matching. The
application of callipers can be used to control matching units that are
considered too far apart. Matching can be performed with or without
replacement and it should be noted that the order of the dataset when
passed to the matching algorithm can play a significant role in how
units are matched when matching without replacement2.</p>
</div>
<div class="section level4">
<h4 id="weighting">Weighting<a class="anchor" aria-label="anchor" href="#weighting"></a>
</h4>
<p>Weighting with the propensity score differs from matching as no units
are pruned from the dataset. This is an advantage over matching in the
sense that all data can remain in the analysis, giving the opportunity
to calculate the average treatment effect on all units (ATE), as opposed
to the ATT.</p>
<p>The formulas for weighting vary depending on the method you want to
employ. The data being analysed, by virtue of its source, may be more
appropriately analysed for the ATT and not the ATE. The formulas for
these weights are:<br><span class="math display">\[ATE,
w(W,x)=\frac{W}{\hat{e}(x)}+\frac{1-W}{1-\hat{e}(x)}\]</span><br><span class="math display">\[ATT,
w(W,x)=W+(1-W)\frac{\hat{e}(x)}{1-\hat{e}(x)}\]</span><br>
where <span class="math inline">\(\hat{e}(x)\)</span> indicates the
estimated propensity score conditional on observed <span class="math inline">\(x\)</span> covariates1,3.</p>
</div>
<div class="section level4">
<h4 id="stratification-aka-subclassification">Stratification (aka Subclassification)<a class="anchor" aria-label="anchor" href="#stratification-aka-subclassification"></a>
</h4>
<p>The stratification method varies from matching and weighting in that
it relies on the use of the propensity score to group units into
quantiles. Once the number of groups has been established, the
literature suggests this is between five and ten depending on the size
of the dataset5, the treatment effect is estimated across the groups and
then averaged to give a mean difference ATE1,3.</p>
</div>
</div>
<div class="section level3">
<h3 id="mahalanobis-matching">Mahalanobis matching<a class="anchor" aria-label="anchor" href="#mahalanobis-matching"></a>
</h3>
<p>The Mahalanobis Distance was developed by Prasanta Chandra
Mahalanobis in 1936 as a means of comparing skulls based on their
measurements. It’s normal application calculates how many standard
deviations a point is from the mean of the specified variables and can
give a good indication of outliers in a group. In matching, the
application is similar but not quite the same. Instead of using the mean
as a comparator, each treated unit is compared, pairwise, with each of
the units in the control group.</p>
<p>The pairwise Mahalanobis Distance is calculated by <span class="math inline">\(D_{ij} = (X_i−X_j)W^{−1}(X_i−X_j)^T\)</span> (<a href="https://stats.stackexchange.com/questions/65705/pairwise-mahalanobis-distances" class="external-link uri">https://stats.stackexchange.com/questions/65705/pairwise-mahalanobis-distances</a>)
where <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> represent the matrix of covariates
for the treated and control groups and <span class="math inline">\(W^{-1}\)</span> is the covariance matrix. Similar
to the Propensity Score, the Mahalanobis Distance reduces a
multidimensional space to a single value representing the distance
between units. A major difference, which will soon be seen, is how the
matches occur between the methods. The Mahalanobis Distance uses the raw
information to calculate the distance between individual units which is
in contrast to the Propensity Score which uses the probability of being
treated to then determine the distance.</p>
</div>
<div class="section level3">
<h3 id="exact-and-coarsened-exact-matching">Exact and coarsened exact matching<a class="anchor" aria-label="anchor" href="#exact-and-coarsened-exact-matching"></a>
</h3>
<p>There are many methods of matching available. Two others that are
similar to the aforementioned are exact matching and coarsened exact
matching.</p>
<p>Exact matching is as it sounds where matches require exactly the same
covariate values to be paired together. This method can be too biased in
its matching and result in a low number of matches but may be suited to
a large dataset where the covariates are discrete or categorical. With
categorical data in particular, there may be some utility in using or
adding exact matching as calculating a “distance” between categorical
groups is often not suitable.</p>
<p>Similar to exact matching, coarsened exact matching (CEM) uses
buckets to group data together based on the covariate values. If you
were looking at a two dimensional plot it would look like a grid. With
CEM, only the grid squares with both treated and control units in them
would be kept. This can be a good trade off between keeping more
observations while still improving covariate balance.</p>
</div>
</div>
<div class="section level2">
<h2 id="gaps-in-understanding">Gaps in Understanding<a class="anchor" aria-label="anchor" href="#gaps-in-understanding"></a>
</h2>
<p>The process and underlying logic of the matching process belies it’s
complexity. The propensity score matching process appears to be straight
forward and require little effort to implement, which is reflected in
its ubiquitous use. However, as King and Neilsen point out, there are
many situations where the use of the propensity score for matching may
not be suitable. Their 2019 paper gives many insights into the use of
the propensity score and why other methods, like CEM and the Mahalanobis
distance, may be better choices. While some of the explanations do make
intuitive sense it can be difficult to decipher the practical
application and implications of the advice.</p>
<div class="section level3">
<h3 id="the-propensity-score-paradox">The propensity score paradox<a class="anchor" aria-label="anchor" href="#the-propensity-score-paradox"></a>
</h3>
<p>King and Neilsen presented a paradox that arises when using the
propensity score for matching2. They were able to demonstrate that, when
compared to other matching distances like the Mahalanobis distance,
matching on the propensity score only approximated a randomised
experiment as opposed to a more efficient fully blocked experiment.</p>
<p>As the matching algorithm proceeds, unmatched observations are pruned
from the dataset. Initially, this reduces the imbalance between the
treated and untreated groups with corresponding improvements to the
estimated treatment effect. However, after a certain threshold is
reached, further pruning the dataset starts to result in more biased
estimates of the treatment effect. This is the PSM paradox.</p>
<p>The authors postulated that this made matching on the propensity
score inferior and were able to demonstrate this concept visually but
were unable to provide mathematical proofs of the implications for real
data.</p>
</div>
<div class="section level3">
<h3 id="model-dependence">Model Dependence<a class="anchor" aria-label="anchor" href="#model-dependence"></a>
</h3>
<p>Another issue that can arise generally with causal inference, but
then also specifically with matching, is model dependence. This is
essentially where the model selection still plays a large role in
predicting the estimate of the treatment effect. This is a problem
because that probably means that matching has not fixed the issue of
confounding and now selection bias needs to be managed as well.</p>
</div>
</div>
<div class="section level2">
<h2 id="statement-of-research-aim">Statement of research aim<a class="anchor" aria-label="anchor" href="#statement-of-research-aim"></a>
</h2>
<p>The aim of this research is to explore some basic matching
methodology and provide an interactive and educational space for people
who want to understand matching and propensity score methods better. The
space will be provided with a ShinyApp that can be accessed publically
and supported by a website and R package stored on Github. This will
allow novices to experiment with different matching methods and more
experienced users to simulate their own matching experiments in the R
environment.</p>
</div>
<div class="section level2">
<h2 id="interactive-applications-to-support-learning">Interactive applications to support learning<a class="anchor" aria-label="anchor" href="#interactive-applications-to-support-learning"></a>
</h2>
<p>Learning by doing has been demonstrated to significantly improve
learning when properly scaffolded, especially as part of Massive Open
Online Courses (MOOCs), which are a key component of the modern
educational tool kit in tertiary settings9. This project is aimed at the
development of an educational tool where users can build an intuitive
understanding of matching methods and compare different methods as
discussed above. Using this educational tool, students will be able to
generate data and visualise the matching process as it unfolds. Users
will also be able to compare the performance of matching, with different
degrees of pruning, to the weighting and stratification applications of
the propensity score that don’t involve pruning, and to matching
approaches using the alternative Mahalanobis distance.</p>
<div class="section level3">
<h3 id="a-few-examples">A few examples<a class="anchor" aria-label="anchor" href="#a-few-examples"></a>
</h3>
<p>RStudio supports the implementation of learnr packages that are used
to provide self guided, tutorial style experiences for users. This
options has had wider uptake in the tech savvy academic community.</p>
<p>There has been a recent growth in the use of interactive learning
websites like <a href="https://www.khanacademy.org" taget="_blank" class="external-link">Kahn
Academy</a>. In this environment users are able to access scaffolded
learning through theoretical concepts coupled with practical
applications. As noted above, this has been found to be an effective
method for providing education.</p>
</div>
</div>
<div class="section level2">
<h2 id="the-space-for-app-development">The space for app development<a class="anchor" aria-label="anchor" href="#the-space-for-app-development"></a>
</h2>
<p>Currently, there is no available interactive options to simulate
matching methods. From an educational perspective, a students
understanding of the matching process is virtually entirely theoretical.
The purpose of the app is to provide educational support and the ability
for for users to play with and visualise how matching occurs. In
addition, this app has been developed as a package which makes easy to
use functions available to more advanced users who may want to
investigate their own data.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Jason Meyer.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
